import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import statistics
from stqdm import stqdm

# C·∫•u h√¨nh trang
st.set_page_config(layout="wide", page_title="PBQC Optimizer (Raw Data)", page_icon="üìà")

st.markdown("## **:blue[PBQC Parameters Optimizer (Fixed Truncation & Raw Data)]**")
st.markdown("Ph·∫ßn m·ªÅm t·ªëi ∆∞u h√≥a tham s·ªë QC d·ª±a tr√™n b·ªánh nh√¢n, s·ª≠ d·ª•ng d·ªØ li·ªáu th√¥ v√† gi·ªõi h·∫°n c·∫Øt c·ªë ƒë·ªãnh.")
st.write("---")

# --- SIDEBAR: UPLOAD & DATA SELECTION ---
with st.sidebar:
    st.header("1. Data Upload")
    uploaded_file = st.file_uploader("Upload file Excel (.xlsx) ho·∫∑c CSV (.csv)", type=['csv', 'xlsx'])

    @st.cache_data
    def load_data(file):
        try:
            df = pd.read_excel(file)
        except:
            df = pd.read_csv(file, sep=None, engine='python')
        return df

    analyte_data = None
    day_data = None

    if uploaded_file is not None:
        df = load_data(uploaded_file)
        
        st.header("2. Column Selection")
        col_result = st.selectbox("Ch·ªçn c·ªôt K·∫øt qu·∫£ x√©t nghi·ªám", tuple(df.columns))
        col_date = st.selectbox("Ch·ªçn c·ªôt Ng√†y/Batch", tuple(df.columns))
        
        # X·ª≠ l√Ω d·ªØ li·ªáu
        analyte_data = df[col_result].dropna().reset_index(drop=True)
        # L·∫•y ng√†y t∆∞∆°ng ·ª©ng v·ªõi d·ªØ li·ªáu k·∫øt qu·∫£ (sau khi dropna)
        day_data = df.loc[analyte_data.index, col_date].reset_index(drop=True)
        
        st.success(f"ƒê√£ t·∫£i {len(analyte_data)} d√≤ng d·ªØ li·ªáu.")
    else:
        st.info("Vui l√≤ng upload d·ªØ li·ªáu ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

# --- MAIN CONTENT ---

if analyte_data is not None:
    # --- INPUT PARAMETERS ---
    st.subheader("üõ†Ô∏è Thi·∫øt l·∫≠p tham s·ªë")
    
    c1, c2, c3, c4 = st.columns(4)
    with c1:
        TEa = st.number_input('**:red[TEa (%)]** - T·ªïng sai s·ªë cho ph√©p', value=5.0, step=0.1, format="%.2f")
    with c2:
        allowable_FPR = st.number_input('**:red[Max FPR (%)]** - T·ª∑ l·ªá d∆∞∆°ng t√≠nh gi·∫£ t·ªëi ƒëa', value=10.0, step=0.1, format="%.1f")
    
    # T√≠nh to√°n min/max g·ª£i √Ω cho truncation
    data_min = float(analyte_data.min())
    data_max = float(analyte_data.max())

    with c3:
        # User t·ª± nh·∫≠p Truncation Limit (Thay v√¨ thu·∫≠t to√°n t·ª± t√¨m)
        lower_trunc = st.number_input('**:blue[Lower Truncation Limit]**', value=data_min, format="%.2f")
    with c4:
        upper_trunc = st.number_input('**:blue[Upper Truncation Limit]**', value=data_max, format="%.2f")

    st.write("---")
    
    # N√∫t ch·∫°y t·ªëi ∆∞u
    if st.button("üöÄ **B·∫Øt ƒë·∫ßu T·ªëi ∆∞u h√≥a (EWMA)**", type="primary"):
        
        # 1. TRUNCATE DATA THEO INPUT C·ª¶A NG∆Ø·ªúI D√ôNG
        mask = (analyte_data >= lower_trunc) & (analyte_data <= upper_trunc)
        clean_data = analyte_data[mask].reset_index(drop=True)
        clean_day = day_data[mask].reset_index(drop=True)
        
        if len(clean_data) < 50:
            st.error("D·ªØ li·ªáu sau khi c·∫Øt l·ªçc qu√° √≠t ƒë·ªÉ ph√¢n t√≠ch. Vui l√≤ng n·ªõi r·ªông kho·∫£ng Truncation Limit.")
            st.stop()

        # T√≠nh Mean/SD tr√™n Raw Data (ƒë√£ l·ªçc)
        mean_val = np.mean(clean_data)
        std_val = np.std(clean_data)
        
        st.info(f"**Th·ªëng k√™ d·ªØ li·ªáu sau l·ªçc:** Mean = {mean_val:.2f}, SD = {std_val:.2f}, N = {len(clean_data)}")

        # 2. CHU·∫®N B·ªä V√íNG L·∫∂P T·ªêI ∆ØU
        # C√°c tham s·ªë c·ªë ƒë·ªãnh v√† bi·∫øn ƒë·ªïi
        error_added_point = 10  # Warm-up period
        max_block_size_limit = 160
        
        # V√≤ng l·∫∑p: Error Rate (0% ƒë·ªÉ t√≠nh FPR, TEa% ƒë·ªÉ t√≠nh ƒê·ªô nh·∫°y)
        sim_errors = [0.0, TEa] 
        
        # V√≤ng l·∫∑p: Control Limits (Sigma multipliers t·ª´ 0.5 ƒë·∫øn 4.0)
        limits_range = np.arange(0.5 * std_val, 4.2 * std_val, 0.5 * std_val)
        
        # V√≤ng l·∫∑p: Block Size (Span c·ªßa EWMA t·ª´ 10 ƒë·∫øn 160)
        block_sizes = np.arange(10, max_block_size_limit, 10)

        # T·ªïng s·ªë l·∫ßn l·∫∑p ƒë·ªÉ hi·ªÉn th·ªã thanh ti·∫øn tr√¨nh
        total_iterations = len(sim_errors) * len(limits_range) * len(block_sizes) * clean_day.nunique()
        
        performance_metrics = {}
        
        # Kh·ªüi t·∫°o dataframe c∆° s·ªü ƒë·ªÉ t√≠nh to√°n nhanh h∆°n
        base_df = pd.DataFrame({'Day': clean_day, 'Data': clean_data})
        
        # --- B·∫ÆT ƒê·∫¶U CH·∫†Y SIMULATION ---
        with stqdm(total=total_iterations, unit='iter', desc="ƒêang t·ªëi ∆∞u h√≥a...") as pbar:
            
            for error_rate in sim_errors:
                # T·∫°o d·ªØ li·ªáu l·ªói gi·∫£ l·∫≠p
                # N·∫øu error_rate = 0 -> D·ªØ li·ªáu g·ªëc -> T√≠nh FPR
                # N·∫øu error_rate = TEa -> D·ªØ li·ªáu l·ªói -> T√≠nh Sensitivity (Ped)
                
                # Copy d·ªØ li·ªáu ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng v√≤ng l·∫∑p sau
                sim_df = base_df.copy()
                
                # Control Limit Loop
                for limit_width in limits_range:
                    UCL = mean_val + limit_width
                    LCL = mean_val - limit_width
                    
                    # Block Size Loop
                    for block_size in block_sizes:
                        
                        alerts = 0
                        detection_indexes = [] # L∆∞u ch·ªâ s·ªë ph√°t hi·ªán l·ªói
                        
                        # Loop qua t·ª´ng ng√†y/batch
                        unique_days = sim_df['Day'].unique()
                        
                        for day in unique_days:
                            day_df = sim_df[sim_df['Day'] == day].copy()
                            day_df = day_df.reset_index(drop=True)
                            
                            # Th√™m l·ªói nh√¢n t·∫°o (Simulation Error)
                            if len(day_df) > error_added_point:
                                # Ch·ªâ th√™m l·ªói v√†o ph·∫ßn sau giai ƒëo·∫°n warm-up
                                day_df.loc[error_added_point:, 'Data'] *= (1 + error_rate / 100)
                            
                            # T√≠nh EWMA
                            ewma = day_df['Data'].ewm(span=block_size, adjust=False).mean()
                            
                            # Check Alerts
                            is_high = ewma >= UCL
                            is_low = ewma <= LCL
                            
                            if is_high.any() or is_low.any():
                                alerts += 1
                                
                                # T√¨m ƒëi·ªÉm ph√°t hi·ªán l·ªói ƒë·∫ßu ti√™n
                                first_high = day_df[(day_df.index >= error_added_point) & is_high].index.min()
                                first_low = day_df[(day_df.index >= error_added_point) & is_low].index.min()
                                
                                detected_idx = None
                                if pd.notna(first_high) and pd.notna(first_low):
                                    detected_idx = min(first_high, first_low)
                                elif pd.notna(first_high):
                                    detected_idx = first_high
                                elif pd.notna(first_low):
                                    detected_idx = first_low
                                    
                                if detected_idx is not None:
                                    detection_indexes.append(detected_idx + 1 - error_added_point)

                            pbar.update(1)
                        
                        # T·ªïng h·ª£p k·∫øt qu·∫£ cho b·ªô tham s·ªë n√†y
                        positive_rate = alerts / len(unique_days)
                        anped = statistics.mean(detection_indexes) if detection_indexes else np.nan
                        mnped = statistics.median(detection_indexes) if detection_indexes else np.nan
                        
                        # Key l∆∞u tr·ªØ: error_rate | limit_width | block_size
                        performance_metrics[f"{error_rate}|{limit_width}|{block_size}"] = {
                            'error_rate': error_rate,
                            'control_limit_width': limit_width,
                            'block_size': block_size,
                            'positive_rate': positive_rate,
                            'ANPed': anped,
                            'MNPed': mnped
                        }

        # --- X·ª¨ L√ù K·∫æT QU·∫¢ ---
        results_df = pd.DataFrame.from_dict(performance_metrics, orient='index')
        
        # T√°ch k·∫øt qu·∫£ th√†nh 2 b·∫£ng: FPR (error=0) v√† Error Detection (error=TEa)
        df_fpr = results_df[results_df['error_rate'] == 0].copy()
        df_ed = results_df[results_df['error_rate'] == TEa].copy()
        
        # ƒê·ªïi t√™n c·ªôt ƒë·ªÉ merge
        df_fpr = df_fpr[['control_limit_width', 'block_size', 'positive_rate']].rename(columns={'positive_rate': 'FPR'})
        df_ed = df_ed[['control_limit_width', 'block_size', 'positive_rate', 'ANPed', 'MNPed']].rename(columns={'positive_rate': 'TPR'})
        
        # Merge d·ª±a tr√™n tham s·ªë (Limit & Block Size)
        merged = pd.merge(df_fpr, df_ed, on=['control_limit_width', 'block_size'])
        
        # T√≠nh ch·ªâ s·ªë Youden (Sensitivity + Specificity - 1) = TPR - FPR
        merged['Youden'] = merged['TPR'] - merged['FPR']
        
        # L·ªçc theo ƒëi·ªÅu ki·ªán ng∆∞·ªùi d√πng: FPR <= Allowable FPR
        allowable_fpr_frac = allowable_FPR / 100.0
        valid_candidates = merged[merged['FPR'] <= allowable_fpr_frac]
        
        best_params = None
        
        if valid_candidates.empty:
            st.warning("Kh√¥ng t√¨m th·∫•y tham s·ªë n√†o th·ªèa m√£n m·ª©c FPR y√™u c·∫ßu. ƒêang hi·ªÉn th·ªã tham s·ªë c√≥ Youden cao nh·∫•t b·∫•t k·ªÉ FPR.")
            best_params = merged.loc[merged['Youden'].idxmax()]
        else:
            # Trong c√°c ·ª©ng vi√™n th·ªèa FPR, ch·ªçn c√°i c√≥ ANPed th·∫•p nh·∫•t (ph√°t hi·ªán l·ªói nhanh nh·∫•t)
            # Ho·∫∑c ch·ªçn Youden cao nh·∫•t. ·ªû ƒë√¢y ∆∞u ti√™n Youden cao nh·∫•t trong nh√≥m FPR h·ª£p l·ªá.
            best_params = valid_candidates.loc[valid_candidates['Youden'].idxmax()]

        # --- HI·ªÇN TH·ªä K·∫æT QU·∫¢ T·ªêI ∆ØU ---
        st.subheader("üèÜ Tham s·ªë t·ªëi ∆∞u nh·∫•t")
        
        res_col1, res_col2 = st.columns(2)
        
        with res_col1:
            st.markdown("##### C·∫•u h√¨nh ƒë·ªÅ xu·∫•t:")
            st.info(f"""
            - **Block Size (Span):** {int(best_params['block_size'])}
            - **Control Limit Width:** ¬±{best_params['control_limit_width']:.4f} (t∆∞∆°ng ƒë∆∞∆°ng {best_params['control_limit_width']/std_val:.2f} SD)
            - **Truncation Limits:** {lower_trunc} - {upper_trunc}
            """)
            
        with res_col2:
            st.markdown("##### Hi·ªáu nƒÉng d·ª± ki·∫øn:")
            st.success(f"""
            - **False Positive Rate (FPR):** {best_params['FPR']*100:.2f}% (Y√™u c·∫ßu: < {allowable_FPR}%)
            - **True Positive Rate (Detection @ {TEa}% error):** {best_params['TPR']*100:.2f}%
            - **ANPed (S·ªë m·∫´u trung b√¨nh ƒë·ªÉ ph√°t hi·ªán l·ªói):** {best_params['ANPed']:.1f}
            - **Youden Index:** {best_params['Youden']:.3f}
            """)
            
        # --- V·∫º BI·ªÇU ƒê·ªí HI·ªÜU NƒÇNG (ANPed vs Error Rate) ---
        st.subheader("üìä Bi·ªÉu ƒë·ªì hi·ªáu nƒÉng (ANPed Curve)")
        
        # Ch·∫°y simulation l·∫°i cho b·ªô tham s·ªë t·ªët nh·∫•t tr√™n m·ªôt d·∫£i error rate r·ªông h∆°n ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì
        plot_errors = np.concatenate([
            np.arange(-1.0 * TEa, 0, 0.2 * TEa), # L·ªói √¢m
            np.arange(0.2 * TEa, 1.2 * TEa, 0.2 * TEa) # L·ªói d∆∞∆°ng
        ])
        
        anped_list = []
        mnped_list = []
        err_list = []
        
        best_block = int(best_params['block_size'])
        best_UCL = mean_val + best_params['control_limit_width']
        best_LCL = mean_val - best_params['control_limit_width']
        
        with st.spinner("ƒêang v·∫Ω bi·ªÉu ƒë·ªì..."):
            for err in plot_errors:
                det_indices = []
                for day in clean_day.unique():
                    d_df = base_df[base_df['Day'] == day].reset_index(drop=True).copy()
                    if len(d_df) > error_added_point:
                        d_df.loc[error_added_point:, 'Data'] *= (1 + err / 100)
                    
                    ewma_vals = d_df['Data'].ewm(span=best_block, adjust=False).mean()
                    
                    # Check breach
                    breaches = (ewma_vals >= best_UCL) | (ewma_vals <= best_LCL)
                    
                    first_idx = d_df[(d_df.index >= error_added_point) & breaches].index.min()
                    
                    if pd.notna(first_idx):
                        det_indices.append(first_idx + 1 - error_added_point)
                
                if det_indices:
                    anped_list.append(statistics.mean(det_indices))
                    mnped_list.append(statistics.median(det_indices))
                else:
                    anped_list.append(None)
                    mnped_list.append(None)
                err_list.append(err)

        # Plotly Graph
        fig = go.Figure()
        fig.add_trace(go.Scatter(x=err_list, y=anped_list, mode='lines+markers', name='ANPed (Average)'))
        fig.add_trace(go.Scatter(x=err_list, y=mnped_list, mode='lines+markers', name='MNPed (Median)', line=dict(dash='dot')))
        
        fig.add_vline(x=0, line_dash="dash", line_color="red", annotation_text="No Error")
        
        fig.update_layout(
            title="T·ªëc ƒë·ªô ph√°t hi·ªán l·ªói (ANPed) theo m·ª©c ƒë·ªô l·ªói",
            xaxis_title="Error Rate (%)",
            yaxis_title="S·ªë m·∫´u b·ªánh nh√¢n (N)",
            template="plotly_white"
        )
        
        st.plotly_chart(fig, use_container_width=True)
